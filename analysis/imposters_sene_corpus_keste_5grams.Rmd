---
title: "R Notebook"
author: "Paschalis Agapitos"
output:
  pdf_document: default
  html_notebook: default
---
# Importing the libraries

```{r}
library(stylo)
```

# Setting working directory

The first step that we need to take is to set the working directory to the folder where this notebook and our dataset are saved. In our case `Analysis` is our working directory.

```{r}
setwd("../analysis/")
getwd()
```

# Preparation of the dataset

The "preparation of the dataset" step consists of six steps.

1)  importing the corpus; at this step we need to make sure that we point out to the correct directory.
2)  tokenizing the corpus, lowering the case of the letters and removing punctuation marks. A very crucial detail on which we should pay attention is the the aspect of the language. Our dataset is written in Latin, so we could either use the `Latin` or `Latin.corr` in the parameter `lang`. Since a lot of manuscripts in Latin do not distinguish the letter `v` from the letter `u`, the second option suits better in this case.
3)  removing the pronouns; this step is optional but since some studies have shown that pronouns are related to the genre or content of a text, it was decided for them to be removed.
4)  extracting the features that we want to use; in our case character tetra-grams and penta-grams will be used.
5)  creating the frequency list
6)  creating the table with frequencies using the frequency list that was created in the previous step.

```{r}
# step 1
raw.corpus <- load.corpus(
  files = "all",
  corpus.dir = "corpus_kestemont",
  encoding = "UTF-8"
)
```

```{r}
# step 2
tokenized.corpus <- txt.to.words.ext(
    raw.corpus,
    corpus.lang = "Latin.corr",
    preserve.case = F,
)
```

```{r}
# step 3
corpus.no.pronouns <- delete.stop.words(
  tokenized.corpus,
  stop.words = stylo.pronouns(corpus.lang = "Latin.corr")
)
```


```{r}
# step 4
corpus.char.pentagrams <- txt.to.features(
  tokenized.text = corpus.no.pronouns,
  features = "c",
  ngram.size = 5
)
```
```{r}
# step 5
features_char_pentagrams <- make.frequency.list(
  corpus.char.pentagrams,
  head = 1000
)
```
```{r}
# step 6
data = make.table.of.frequencies(
  corpus = corpus.char.pentagrams,
  features = features_char_pentagrams,
  relative = T
)
```
# Implementation of `imposters` method
In order to implement `imposters` to the data that we have prepared in the previous steps we first need to find in which rows the disputed texts are. This will help us when we need to split the dataset into reference and test set; we could do that just by referring to the number of the row where the texts under investigation are.
Since are our dataset contains more than 999 rows we need to set the option for printing more than 999 rows. We have 1814 text samples, so if we set `max.option = 2000` it will output all the rows.
```{r}
options(max.print=2000)
rownames(data)
```

We can see that Octavia is at the *1364th row* and Hercules Oetaeus at the *1293th row*.

```{r}
# help("imposters")
# according to Koppel and Winter(2014) the number of iteration does not change the result of the classification, thus iterations = 100 (default value)

imposters.octavia <- imposters(
  reference.set = data[-c(1364), 1:200],
  test = data[1364, 1:200],
  iterations = 100,
  distance = 'eder'
)
```
The next phase is to apply the same method on the other disputed text, Hercules Oetaeus.
```{r}
imposters.hercoet <- imposters(
  reference.set = data[-c(1293), 1:200],
  test = data[1293, 1:200],
  iterations = 100,
  distance = 'eder'
)
```

# Optimize

The `imposters` method comes with an in-built function that optimizes the hyperparameters used in the General Imposters method. In other words it tries to define the grey area where these result could be valid or not.

```{r}
# help("imposters.optimize")
# imposters.optimize(data)
```
```{r}
print(imposters.octavia)
print(imposters.hercoet)
```
