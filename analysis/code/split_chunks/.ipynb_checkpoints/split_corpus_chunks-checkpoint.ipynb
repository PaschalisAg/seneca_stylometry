{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f18289a-dc3c-4d44-bd04-5fe87736bfb5",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "The code below defines a function `extract_chunks` that processes Latin texts by splitting them into non-overlapping chunks of $n$ length. \n",
    "Here is a step-by-step explanation of what the code does:\n",
    "\n",
    "1. **Import Necessary Libraries**:\n",
    "   - `cltk`: Used for the Latin tokenizer.\n",
    "   - `re`, `os`, `glob`: Used for file operations and regular expressions.\n",
    "   - `pandas`, `numpy`: Imported but not used in the code.\n",
    "   \n",
    "2. **Function Definition**: `extract_chunks`\n",
    "   - **Purpose**: To split Latin texts into non-overlapping chunks based on the specified chunk size.\n",
    "   - **Parameters**:\n",
    "     - `directory_to_read`: Directory containing the text files to process.\n",
    "     - `directory_to_write`: Directory where the processed chunks will be saved.\n",
    "     - `threshold_to_slice`: Token count threshold above which texts will be split into chunks.\n",
    "     - `chunk_size`: The size of each chunk in tokens.\n",
    "\n",
    "3. **Helper Functions**:\n",
    "   - `count_files(directory)`: Counts the number of `.txt` files in a directory.\n",
    "   - `read_file(filepath)`: Reads the content of a file.\n",
    "   - `preprocess(text)`: Removes Arabic numbers and non-word characters from the text.\n",
    "   - `tokenize_latin_text(text)`: Lowercases and tokenizes Latin text using CLTK's Latin tokenizer.\n",
    "\n",
    "4. **Directory Check**:\n",
    "   - Ensures that the directory to save the chunks exists. If not, it creates the directory.\n",
    "\n",
    "5. **Processing Each File**:\n",
    "   - Iterates through each `.txt` file in the `directory_to_read`.\n",
    "   - Tokenizes the text content.\n",
    "   - If the number of tokens exceeds the `threshold_to_slice`, splits the text into non-overlapping chunks of `chunk_size` tokens.\n",
    "   - Writes each chunk to a new file with a modified name indicating the chunk number.\n",
    "   - If the number of tokens is below the threshold, writes the text as is to the output directory.\n",
    "\n",
    "6. **Summary Print Statement**:\n",
    "   - Prints a summary message indicating the number of text samples written to the `directory_to_write`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7986d82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T19:23:51.205435Z",
     "start_time": "2023-04-30T19:23:47.652827Z"
    }
   },
   "outputs": [],
   "source": [
    "import cltk\n",
    "from cltk.tokenizers import LatinWordTokenizer\n",
    "from glob import glob\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_chunks(directory_to_read, directory_to_write, threshold_to_slice, chunk_size):\n",
    "    \"\"\"\n",
    "    The function `extract_chunks` slices texts in non-overlapping chunks based on the specified chunk size.\n",
    "    Before extracting the chunks, it performs brief preprocessing, such as removing Arabic numbers,\n",
    "    lowercasing, and tokenizing the texts.\n",
    "\n",
    "    Note: This function is designed for Latin texts using CLTK's Latin tokenizer.\n",
    "\n",
    "    Parameters:\n",
    "        directory_to_read (str): The directory containing the texts to slice.\n",
    "        directory_to_write (str): The directory to write the results. If it doesn't exist, it will be created.\n",
    "        threshold_to_slice (int): The token count threshold above which texts will be split into chunks.\n",
    "        chunk_size (int): The size of each non-overlapping chunk.\n",
    "\n",
    "    Returns:\n",
    "        A directory with the txt files provided split into non-overlapping chunks of 500 tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    def count_files(directory):\n",
    "        \"\"\"Count the number of .txt files in the given directory.\"\"\"\n",
    "        count = 0\n",
    "        for path in os.scandir(directory):\n",
    "            if os.path.isfile(os.path.join(directory, path)) and path.name.endswith(\".txt\"):\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def read_file(filepath):\n",
    "        \"\"\"Read the content of a file.\"\"\"\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "\n",
    "    def preprocess(text):\n",
    "        \"\"\"Remove Arabic numbers from the text and return the cleaned text.\"\"\"\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        return text\n",
    "\n",
    "    def tokenize_latin_text(text):\n",
    "        \"\"\"Lowercase and tokenize Latin text.\"\"\"\n",
    "        latin_tokenizer = LatinWordTokenizer()\n",
    "        text = preprocess(text.lower())\n",
    "        tokens = latin_tokenizer.tokenize(text)\n",
    "        return tokens\n",
    "\n",
    "    # ensure the directory to write exists\n",
    "    if not os.path.exists(directory_to_write):\n",
    "        os.makedirs(directory_to_write)\n",
    "        print(\"Directory successfully created!\")\n",
    "    else:\n",
    "        print(f\"Directory {directory_to_write} already exists!\")\n",
    "\n",
    "    # process each file in the directory\n",
    "    for file_name in os.listdir(directory_to_read):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_to_read, file_name)\n",
    "            tokens = tokenize_latin_text(read_file(file_path))\n",
    "\n",
    "            if len(tokens) > threshold_to_slice:\n",
    "                chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    chunk_text = \" \".join(chunk)\n",
    "                    chunk_file_name = f\"{file_name[:-4]}_chunk{i + 1}.txt\"\n",
    "                    with open(os.path.join(directory_to_write, chunk_file_name), \"w\", encoding='utf-8') as f:\n",
    "                        f.write(chunk_text)\n",
    "            else:\n",
    "                text = \" \".join(tokens)\n",
    "                with open(os.path.join(directory_to_write, file_name), \"w\", encoding='utf-8') as f:\n",
    "                    f.write(text)\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Every file has been written successfully.\n",
    "    The new directory (path={directory_to_write}) contains {count_files(directory_to_write)} text samples.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbd12517",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T19:24:24.405655Z",
     "start_time": "2023-04-30T19:24:13.826234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ../../corpora/corpus_chunks/ already exists!\n",
      "\n",
      "    Every file has been written successfully.\n",
      "    The new directory (path=../../corpora/corpus_chunks/) contains 1196 text samples.\n",
      "CPU times: user 10.9 s, sys: 602 ms, total: 11.5 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "directory_to_read = \"../../corpora/corpus_test_chunks/\"  # get the working directory\n",
    "# set the directory where you want to write the results\n",
    "directory_to_write = \"../../corpora/corpus_chunks/\"\n",
    "extract_chunks(directory_to_read=directory_to_read,\n",
    "               directory_to_write=directory_to_write, threshold_to_slice=500, chunk_size=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
