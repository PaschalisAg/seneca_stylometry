{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a7986d82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:27:31.702308Z",
     "start_time": "2023-03-23T16:27:31.676351Z"
    }
   },
   "outputs": [],
   "source": [
    "import cltk\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from glob import glob\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_chunks(directory_to_read, directory_to_write, threshold_to_slice, chunk_size):\n",
    "    \"\"\"\n",
    "    The function `extract_chunks` slices in non-overlapping chunks the texts \n",
    "    with respect to the indicated chunk size.\n",
    "    Before the extraction of the non-overlapping chunks there is a brief preprocessing, \n",
    "    such as removing the arabic numbers\n",
    "    from the texts, lowering, and tokenizing the texts.\n",
    "\n",
    "    One mandatory requirement is for the texts to be in Latin because it uses CLTK's tokenizer for Latin.\n",
    "\n",
    "    It takes three argument:\n",
    "\n",
    "        1) `directory_to_read`: the directory where you want the function to check for the texts to slice.\n",
    "        This might be the same directory as the one where you are writing the results or a different location.\n",
    "\n",
    "        2) `threshold_to_slice`: this threshold is translated as the limit after which the function\n",
    "        will start splitting the text into chunks. For instance, if it is set to 1500, \n",
    "        then it will start splitting texts that are longer than 1500 tokens, \n",
    "        otherwise it won't do anything to them.\n",
    "\n",
    "        3) `chunk_size`: the size of the chunk you want to write in a txt file. \n",
    "        The chunks will be non-overlapping to each other \n",
    "\n",
    "        4) `directory_to_write`: the directory where you want to write the results. \n",
    "        If the directory does not exist, then the function will create a new directory \n",
    "        to the appointed location. \n",
    "        If it exists already, then the function won't do anything.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def count_files_dir(directory_to_read):\n",
    "        count = 0\n",
    "    # iterate through every filepath in a directory\n",
    "        for path in os.scandir(directory_to_read):\n",
    "            # include only txt files\n",
    "            if os.path.isfile(os.path.join(directory_to_read, path)) and path.name.endswith(\".txt\"):\n",
    "                count += 1  # incerement the counter every time the condition is satisfied\n",
    "        return count\n",
    "\n",
    "    def read_file(directory):  # read the file\n",
    "        with open(directory, 'r', encoding='utf-8') as inp:\n",
    "            text = inp.read()\n",
    "        return text\n",
    "\n",
    "    # function to remove the arabic numbers that may exist as number lines within the text\n",
    "    def preprocess(text):\n",
    "        # keeps only words and whitespaces\n",
    "        text = re.sub(r'[^\\w\\s]', '', read_file(text))\n",
    "        return text  # returns the text without the numbers\n",
    "\n",
    "    # function to tokenize latin texts\n",
    "    def tokenize_latin_text(text):\n",
    "        latin_tokenizer = WordTokenizer(\"latin\")  # initialize CLTK tokenizer\n",
    "        text = latin_tokenizer.tokenize(preprocess(\n",
    "            text.lower()))  # lowers and tokenizes the text\n",
    "        return text  # return a list class\n",
    "\n",
    "    # check if the directory already exists\n",
    "    if not os.path.exists(directory_to_write):\n",
    "        os.mkdir(directory_to_write)  # if not, then create\n",
    "        print(\"Directory successfully created!\")  # print message\n",
    "    else:  # if yes, then say that it already exists and return the path\n",
    "        print(f\"Directory in {directory_to_write} already exists!\")\n",
    "\n",
    "    # Loop through each file in the directory\n",
    "    for file_name in os.listdir(directory_to_read):\n",
    "        # check if files is a txt file (ignore binary files or system files)\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            tokens = tokenize_latin_text(\n",
    "                directory_to_read + file_name)  # tokenize every text\n",
    "\n",
    "            # check if the text is longer than the appointed limit to slice\n",
    "            # texts above this limit will be split into chunks of chunk_size size\n",
    "            # satire 5 has 1327 tokens, thus the slice will be set to 1350\n",
    "            if len(tokens) > threshold_to_slice:\n",
    "                # Split the text into non-overlapping chunks of 1000 tokens each\n",
    "                chunks = [tokens[i:i+chunk_size]\n",
    "                          for i in range(0, len(tokens), chunk_size)]\n",
    "                # write each chunk to a new file with \"_chunk#\" added to the file name\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    chunk_text = \" \".join(chunk)\n",
    "                    chunk_file_name = f\"{file_name[:-4]}_chunk{i+1}.txt\"\n",
    "                    with open(os.path.join(directory_to_write, chunk_file_name), \"w\") as f:\n",
    "                        f.write(chunk_text)\n",
    "\n",
    "            # if the length is less than 1350 tokens, then write as it is by preserving its current filename\n",
    "            else:\n",
    "                text = \" \".join(tokens)  # text as a string\n",
    "            # write the text to a new file\n",
    "                with open(os.path.join(directory_to_write, file_name), \"w\") as f:\n",
    "                    f.write(text)\n",
    "\n",
    "    # do some checking to see where your files have been written and how many text sample have been generated\n",
    "    print(f\"\"\"\n",
    "    Every file has been written successfully. \n",
    "    Your new directory (path={directory_to_write}) contains {count_files_dir(directory_to_write)} text samples.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "49fa5fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:34:38.543528Z",
     "start_time": "2023-03-23T16:34:38.537402Z"
    }
   },
   "outputs": [],
   "source": [
    "# help(extract_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fbd12517",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-23T16:27:40.959200Z",
     "start_time": "2023-03-23T16:27:32.981059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory in /Users/paschalis/Documents/MA_DH/Thesis/seneca_stylometry/collection_data/verse_corpus_chunks/ already exists!\n",
      "\n",
      "    Every file has been written successfully. \n",
      "    Your new directory (path=/Users/paschalis/Documents/MA_DH/Thesis/seneca_stylometry/collection_data/verse_corpus_chunks/) contains 632 text samples.\n"
     ]
    }
   ],
   "source": [
    "directory_to_read = os.getcwd() + \"/corpus_test_chunks/\"  # get the working directory\n",
    "# set the directory where you want to write the results\n",
    "directory_to_write = os.getcwd() + \"/verse_corpus_chunks/\"\n",
    "extract_chunks(directory_to_read=directory_to_read,\n",
    "               directory_to_write=directory_to_write, threshold_to_slice=1350, chunk_size=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
