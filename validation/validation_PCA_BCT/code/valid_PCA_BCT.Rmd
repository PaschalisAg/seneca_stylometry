---
title: 'Validation: Seneca stylometry'
author: "Paschalis Agapitos"
output: pdf_document
---

```{r}
# install.packages("stylo")
```

# Introduction

Aim of this notebook is to validate the methods used in the paper *A Stylometric Analysis of Seneca's Disputed Plays: Authorship Verification of Octavia and Hercules Oetaeus*.

A different version of the main anaysis dataset will be used now. The dataset is contained in a folder called `validation_corpus`. It contains 28 texts in verse written by three authors (in total 287138 tokens). The authors used in this corpus are:

-   Publius Ovidius Naso (henceforward: **Ovid**)
    -   Ars Amatoria
    -   Epistulae
    -   Fasti
    -   Ibis
    -   Medicamina Faciei femineae
    -   Metamorphoses
    -   Ex Ponto
    -   Remedia Amoris
    -   Tristia
-   Aulus Persius Flaccus (henceforward: **Persius**
    -   The six books of *Satires*
-   Publius Papinius Statius (henceforward: **Statius**)
    -   The 12 books of *Thebaid*

To validate the methods we selected one text from each author (i.e., in total three texts) and we renamed them with the following format: `unknown{n}.txt`. The authors to validate the methods are the following ones:

-   *Amores* by Ovid (i.e., `unknown0.txt`)
-   *Thebaid* book 1 by Statius (i.e., `unknown1.txt`)
-   *Satire* 4 by Persius (i.e., `unknown2.txt`)

The first two texts were randomly chosen to be tested. However, the last one is the trickiest one because it consists of only 342 tokens.

In this notebook we will apply Principal Component Analysis (henceforward: PCA) and Bootstrap Consensus Tree (henceforward: BCT); for the former we will use a covariance and a correlation matrix to visualise the results. The same preprocessing step will be applied to every text that is used here and the results will be generated using Most Frequent Characters (henceforward MFCs) tetragrams and pentagrams; these number of n-grams will be applied to each one of the aforementioned methods and their variations.

```{r}
library(stylo)
# library(gplots)
# library(pheatmap)
```

# Setting the working directory

Our working directory is set to `validation_PCA_BCT`. This directory holds the data and the code to validate the PCA method and the BCT method.

```{r}
setwd('../../validation_PCA_BCT/')
getwd()
```

# Importing the corpus and tokenization

In this step we import the corpus that we are going to use and consequently we tokenize it. The tokenization follows the rules of the parameter `Latin.corr`. This is done because a lot of texts do not distinguish "u/v" and by setting this parameter to `Latin.corr` we take care of this variation in the letters. Furthermore, we know that since we use texts from the [Perseus Digital Library](http://www.perseus.tufts.edu/hopper/) that the distinction between "u/v" should be addressed. Lastly, we change uppercase letters to lowercase because we need to further minimize the variations between words.

```{r}
# load the corpus for the validation of PCA
raw.corpus <- load.corpus(files = "all", corpus.dir = "../../validation_corpora/validation_corpus_PCA/",
                          encoding = "UTF-8")

tokenized.corpus <- txt.to.words.ext(raw.corpus, corpus.lang = "Latin.corr", 
                                     preserve.case = F) # break texts into tokens

```

# Remove the pronouns

It was decided to remove the pronouns, since some pronouns are connected to the genre of the text.

```{r}
corpus.no.pronouns <- delete.stop.words(tokenized.corpus,
                                        stop.words = stylo.pronouns(corpus.lang = "Latin"))
# the list with the pronouns removed
stylo.pronouns(corpus.lang = "Latin")
# summary(corpus.no.pronouns)
```

# Character 4-grams

## Extracting the features (character 4-grams)

The final step before proceeding to the application of the methods to the corpus is to extract the features that we want to use and add them to a table with frequencies. In our case, we want to extract character 4-grams.

```{r}
corpus.char.4.grams <- txt.to.features(corpus.no.pronouns, 
                                       features = "c", 
                                       ngram.size = 4) # break the text into character 4-grams

frequent.features.4grams <- make.frequency.list(corpus.char.4.grams, 
                                                head = 2000) # extract the features

freqs.4grams <- make.table.of.frequencies(corpus.char.4.grams,
                                          features = frequent.features.4grams, 
                                          relative = T) # relative=True to compute the relative frequency
```

# Methods - Character 4-grams

## Apply Principal Component Analysis

### Principal Component Analysis - Correlation matrix (MFCs 4grams)

In this experiment we will apply Principal Component Analysis (henceforward: PCA) using a correlation matrix to visualise the results. The features used in this experiment are Most Frequent Characters (henceforward: MFCs) 4-grams. We will look at the top 100 to 1500 MFCs 4-grams with an increment of 100 in each iteration (no culling will be specified because we want to obtain a sufficient number of features in each iteration (given this corpus, if we set culling to 100% we obtain only 33 MFC)).

Eder's Delta will be used as a distance metric.

```{r}
# PCA correlation - top 100-2000-100 incr.100 MFCs 4-grams
results_pca_4grams_cor = stylo(frequencies = freqs.4grams, 
                               analysis.type = "PCR",
                               mfw.min = 100, mfw.max = 2000, increment=100, 
                               distance.measure = "wurzburg", # Cosine Delta
                               custom.graph.title = "Who is the author?", # title of the plot
                               pca.visual.flavour="classic", # flavour of the PCA plot
                               write.pdf.file=T, # write the results into png files
                               gui = T) # gui = True to double-check the parameters
```
## Apply Bootstrap Consensus Tree - MFC 4-grams

### Load and prepare corpus for the validation of BCT

Since a different corpus is used for the validation of BCT we need to load the correct one and prepare it for the analysis. The same preprocessing steps will be followed for the analysis.

```{r}
# load the corpus for the validation of BCT
raw.corpus <- load.corpus(files = "all", corpus.dir = "../../../validation/validation_corpora/validation_corpus_BCT/",
                          encoding = "UTF-8")

tokenized.corpus <- txt.to.words.ext(raw.corpus, corpus.lang = "Latin.corr", 
                                     preserve.case = F) # break texts into tokens

```

# Remove the pronouns

It was decided to remove the pronouns, since some pronouns are connected to the genre of the text.

```{r}
corpus.no.pronouns <- delete.stop.words(tokenized.corpus,
                                        stop.words = stylo.pronouns(corpus.lang = "Latin"))
# the list with the pronouns removed
stylo.pronouns(corpus.lang = "Latin")
# summary(corpus.no.pronouns)
```

# Character 4-grams

## Extracting the features (character 4-grams)

The final step before proceeding to the application of the methods to the corpus is to extract the features that we want to use and add them to a table with frequencies. In our case, we want to extract character 4-grams.

```{r}
corpus.char.4.grams <- txt.to.features(corpus.no.pronouns, 
                                       features = "c", 
                                       ngram.size = 4) # break the text into character 4-grams

frequent.features.4grams <- make.frequency.list(corpus.char.4.grams, 
                                                head = 5000) # extract the features

freqs.4grams <- make.table.of.frequencies(corpus.char.4.grams,
                                          features = frequent.features.4grams, 
                                          relative = T) # relative=True to compute the relative frequency
```

```{r}
# BCT 4grams - top 100-2000-100 MFC 4 grams - consensus strength 0.5
bct.results.4grams_100_2000MFC = stylo(frequencies = freqs.4grams, # MFCs char 4grams
                                       distance.measure="wurzburg", # Cosine Delta
                                       analysis.type = "BCT", # Bootstrap Consensus Tree
                                       mfw.min = 100, mfw.max = 2000, increment = 100, # frequency band used
                                       custom.graph.title="Who is the author?", # title of the graph
                                       write.pdf.file=T, # write results into png files
                                       gui = TRUE) # graphic user interface
```

# Conclusions

From the methods their variants ran above, we can observe that all of the "unknown" texts have been successfully attributed to the "correct" author.

-   `unknown0.txt` (i.e., Amores) to Ovid
-   `unknown1.txt` (i.e., Thebaid book 1) to Statius
-   `unknown2.txt` (i.e., Satire 4) to Persius

Especially the last case, *Satire* 4 by Persius is an interesting case. The length of the text is miniscule compared to the other texts in the corpus; it consists of only 342 tokens, thus the distance from the other texts. The other "not-so-tricky" case is the *Medicamina Faciei Femineae* by Ovid which has only 613 tokens (i.e., the second shortest text in our validation dataset).
