


%%time

import os
import csv
import string
from cltk.data.fetch import FetchCorpus
from cltk.sentence.lat import LatinPunktSentenceTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# import the Latin model for the sentence tokenizer
corpus_downloader = FetchCorpus(language='lat')
corpus_downloader.import_corpus('lat_models_cltk')
corpus_downloader.import_corpus('latin_training_set_sentence_cltk')

# initialize CLTK sentence tokenizer for Latin
sentence_tokenizer = LatinPunktSentenceTokenizer(strict=True)

# corpus of texts
directory_path = '../../corpora/corpus_imposters/'

# directory to write the results if it does not exist
results_directory = os.path.join('..', 'lines-similarity', 'results_line_sim_cosine')
os.makedirs(results_directory, exist_ok=True)

# disputed texts (O, HO)
disputed_texts = ['sen_oct.txt', 'sen_her_o.txt']

# dictionary to save sentences for each play
play_sentences = {}

# dictionary to store the count of removed sentences for each disputed play
removed_sentence_count = {}

# function to read files
def read_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return file.read()

# function to tokenize sentences
def tokenize_sentences(text):
    # tokenize into sentences
    sentences = sentence_tokenizer.tokenize(text)
    # remove punctuation and lowercase each sentence
    sent_without_punct = [sentence.translate(str.maketrans('','', string.punctuation)).lower() for sentence in sentences]    
    return sent_without_punct

# function to compare sentences using cosine similarity
def compare_sentences(play1, play2, threshold=.6):
    '''
    Convert sentences into vectors and compare them.
    - `strip_accents='unicode'`: removes accents from characters using Unicode
    - `lowercase=True`:converts all characters to lowercase
    - `analyzer='char'`: analyses the input as a sequense of characters
    - `ngram_range=(4,4)`: considers 4-grams (i.e., sequence of 4 characters if combined with analyser) as features
    '''
    vectorizer = TfidfVectorizer(strip_accents='unicode', lowercase=True, analyzer='char', ngram_range=(4,4))
    vectors = vectorizer.fit_transform(play1 + play2)
    
    # compute cosine similarity by comparing the vectors
    similarities = cosine_similarity(vectors[:len(play1)], vectors[len(play1):])

    # find similar sentences
    similar_lines = [] # list to save similar sentences
    for i in range(len(play1)):
        for j in range(len(play2)):
            if similarities[i, j] >= threshold: # return sentences that are above the threshold 
                similar_lines.append((i, j, similarities[i, j], play1[i], play2[j]))

    return similar_lines

# process each file in the directory
for filename in os.listdir(directory_path):
    file_path = os.path.join(directory_path, filename)

    # check if the file is a Senecan play and not the disputed
    if filename.startswith("sen_") and filename not in disputed_texts:
        # clean filename to make the result more readable
        clean_filename = os.path.splitext(os.path.basename(filename))[0].replace('_', ' ').capitalize()
        # read the file
        text = read_file(file_path)
        # split into sentences
        sentences = tokenize_sentences(text)
        # save the results into the dictionary
        play_sentences[clean_filename] = sentences

# process disputed plays (follow the same steps as for the originals)
for disputed_text in disputed_texts:
    # clean filename to make results more readable
    disputed_text_name = os.path.splitext(os.path.basename(disputed_text))[0].replace('_', ' ').capitalize()
    disputed_path = os.path.join(directory_path, disputed_text)
    # read the contents of the file for each disputed text
    disputed_text_content = read_file(disputed_path)
    # split into sentences (for the disputed texts)
    disputed_sentences = tokenize_sentences(disputed_text_content)

    # compare sentences with each Senecan play using cosine similarity
    for senecan_play, senecan_sentences in play_sentences.items():
        similar_lines = compare_sentences(disputed_sentences, senecan_sentences)

        # save results to CSV with a unique name for each combination
        clean_senecan_play = os.path.splitext(os.path.basename(senecan_play))[0]
        csv_filename = os.path.join(results_directory, f'similarity_{disputed_text_name}_vs_{clean_senecan_play}_res.csv')
        with open(csv_filename, 'w', encoding='utf-8', newline='') as csvfile:
            csv_writer = csv.writer(csvfile)
            csv_writer.writerow(['Disputed Sentence Index', 'Senecan Sentence Index', 'Similarity', 'Disputed Sentence', 'Senecan Sentence'])

            for i, j, similarity, disputed_sentence, senecan_sentence in similar_lines:
                csv_writer.writerow([i, j, similarity, disputed_sentence, senecan_sentence])

        print(f"Examples for {disputed_text_name} vs {clean_senecan_play}:")
        # random_samples = random.sample(similar_lines, min(5, len(similar_lines)))
        for i, j, similarity, disputed_sentence, senecan_sentence in similar_lines:
            print(f"Similarity: {similarity:.4f}")
            print(f"Disputed Sentence: {disputed_sentence}")
            print(f"Senecan Sentence: {senecan_sentence}")

        # count and store the number of removed sentences
        removed_sentence_count[(disputed_text_name, clean_senecan_play)] = len(similar_lines)

        # remove similar sentences from disputed text
        for i, _, _, _, _ in similar_lines:
            disputed_sentences[i] = ""  # Replace similar sentences with an empty string

    # Calculate and print the percentage of removed sentences
    total_sentences = len(disputed_sentences)
    removed_sentences = sum(removed_sentence_count.values())
    percentage_removed = (removed_sentences / total_sentences) * 100
    print(f"Total Sentences in {disputed_text_name}: {total_sentences}")
    print(f"Removed Sentences: {removed_sentences}")
    print(f"Percentage Removed: {percentage_removed:.2f}%")

    # write modified disputed text to a new file
    modified_disputed_path = os.path.join(results_directory, f'modified_{disputed_text_name}.txt')
    with open(modified_disputed_path, 'w', encoding='utf-8') as modified_file:
        modified_file.write('\n'.join([sentence for sentence in disputed_sentences if sentence]))

    print(f"Modified disputed text saved to: {modified_disputed_path}")
