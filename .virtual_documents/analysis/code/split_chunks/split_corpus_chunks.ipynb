


import cltk
from cltk.tokenizers import LatinWordTokenizer
from glob import glob
import re
import os
import pandas as pd
import numpy as np

def extract_chunks(directory_to_read, directory_to_write, threshold_to_slice, chunk_size):
    """
    The function `extract_chunks` slices texts in non-overlapping chunks based on the specified chunk size.
    Before extracting the chunks, it performs brief preprocessing, such as removing Arabic numbers,
    lowercasing, and tokenizing the texts.

    Note: This function is designed for Latin texts using CLTK's Latin tokenizer.

    Parameters:
        directory_to_read (str): The directory containing the texts to slice.
        directory_to_write (str): The directory to write the results. If it doesn't exist, it will be created.
        threshold_to_slice (int): The token count threshold above which texts will be split into chunks.
        chunk_size (int): The size of each non-overlapping chunk.

    Returns:
        A directory with the txt files provided split into non-overlapping chunks of 500 tokens.
    """

    def count_files(directory):
        """Count the number of .txt files in the given directory."""
        count = 0
        for path in os.scandir(directory):
            if os.path.isfile(os.path.join(directory, path)) and path.name.endswith(".txt"):
                count += 1
        return count

    def read_file(filepath):
        """Read the content of a file."""
        with open(filepath, 'r', encoding='utf-8') as file:
            return file.read()

    def preprocess(text):
        """Remove Arabic numbers from the text and return the cleaned text."""
        text = re.sub(r'[^\w\s]', '', text)
        return text

    def tokenize_latin_text(text):
        """Lowercase and tokenize Latin text."""
        latin_tokenizer = LatinWordTokenizer()
        text = preprocess(text.lower())
        tokens = latin_tokenizer.tokenize(text)
        return tokens

    # ensure the directory to write exists
    if not os.path.exists(directory_to_write):
        os.makedirs(directory_to_write)
        print("Directory successfully created!")
    else:
        print(f"Directory {directory_to_write} already exists!")

    # process each file in the directory
    for file_name in os.listdir(directory_to_read):
        if file_name.endswith(".txt"):
            file_path = os.path.join(directory_to_read, file_name)
            tokens = tokenize_latin_text(read_file(file_path))

            if len(tokens) > threshold_to_slice:
                chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]
                for i, chunk in enumerate(chunks):
                    chunk_text = " ".join(chunk)
                    chunk_file_name = f"{file_name[:-4]}_chunk{i + 1}.txt"
                    with open(os.path.join(directory_to_write, chunk_file_name), "w", encoding='utf-8') as f:
                        f.write(chunk_text)
            else:
                text = " ".join(tokens)
                with open(os.path.join(directory_to_write, file_name), "w", encoding='utf-8') as f:
                    f.write(text)

    print(f"""
    Every file has been written successfully.
    The new directory (path={directory_to_write}) contains {count_files(directory_to_write)} text samples.""")


%%time
directory_to_read = "../../corpora/corpus_test_chunks/"  # get the working directory
# set the directory where you want to write the results
directory_to_write = "../../corpora/corpus_chunks/"
extract_chunks(directory_to_read=directory_to_read,
               directory_to_write=directory_to_write, threshold_to_slice=500, chunk_size=500)
