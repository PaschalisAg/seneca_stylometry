import os
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter


sns.set_palette('colorblind')


def read_corpus(directory):
    corpus = ""
    for filename in os.listdir(directory):
        if filename.endswith('.txt'):
            filepath = os.path.join(directory, filename)
            with open(filepath, 'r', encoding='utf-8') as file:
                text = file.read()
                corpus += text + " "
    return corpus


def vectorize_texts(max_features, analyzer, ngram_range, corpus_filename,
                    reverse):
    """
    Help on function vectorize_texts:

    - `max_features`: the number of features you want the matrix to have. E.g., if max_features=2000,
    then only the top 2000 most frequent features will be returned.

    - `analyzer`: {'word', 'char', 'char_wb'} or callable, default='word'
    Whether the feature should be made of word n-gram or character n-grams.
    Option 'char_wb' creates character n-grams only from text inside
    word boundaries; n-grams at the edges of words are padded with space.

    - `ngram_range`: tuple (min_n, max_n), default=(1, 1)
    The lower and upper boundary of the range of n-values for different
    word n-grams or char n-grams to be extracted. All values of n such
    such that min_n <= n <= max_n will be used. For example an
    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means
    unigrams and bigrams, and ``(2, 2)`` means only bigrams.
    Only applies if ``analyzer`` is not callable.

    - `corpus_to_vectorize`: a corpus of several texts that has been combined in a single string.
    See function read_and_combine.

    - `reverse`: {True, False}
    if set to True, it returns the features sorted in descending order.
    If False, then features are sorted in ascending order.
    """

    corpus = read_corpus(corpus_filename)

    # use CountVectorizer to transform the corpus into a matrix of charcter 4-gram counts
    # return only the first 2500 character 4grams with the highest frequency | saves some memory
    vectorizer = CountVectorizer(max_features=max_features,
                                 analyzer=analyzer,
                                 ngram_range=ngram_range)
    X = vectorizer.fit_transform([corpus])

    # convert the matrix to a list of tuples containing the n-grams and their counts
    # toarray(): returns a dense matrix instead of a sparse matrix
    ngram_counts = list(zip(vectorizer.get_feature_names_out(),
                            X.toarray()[0]))

    # sort the list in descending order of the counts:
    ngram_counts_sorted = sorted(ngram_counts,
                                 key=lambda x: x[1],
                                 reverse=reverse)

    # extract the counts and plot them as a line graph:
    counts = [count for word, count in ngram_counts_sorted]

    return counts, ngram_counts_sorted


counts, ngram_counts_sorted = vectorize_texts(
    max_features=10000,
    analyzer='char',
    ngram_range=(4, 4),
    corpus_filename='../../corpora/corpus_imposters/',
    reverse=True)


os.makedirs('../../results/freq_dist_res/', exist_ok=True)

save_dir = '../../results/freq_dist_res/'
# plot counts into a lineplot
ax = sns.lineplot(counts)


plt.axvline(2000, color='r', linestyle='dotted', ymax=max(counts))
ax.set_title(
    "Frequency Distribution\nCharacter 4grams (Verse Corpus Imposters)")
ax.set_xlabel("Character 4gram Index")
ax.set_ylabel("Frequency")
plt.savefig(os.path.join(save_dir, 'freq_dist_char_4grams.pdf'), dpi=300)
plt.show()


import pprint

print("Top 100 MFC 4grams along with their overall frequency:\n")
pprint.pprint(ngram_counts_sorted[:100], width=110, compact=True)



