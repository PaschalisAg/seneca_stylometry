import os
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_palette('colorblind') # set colorblind palette
from collections import Counter

def read_corpus(directory):
    """
    Reads all .txt files in the specified directory and combines them into a single string.

    Parameters:
    directory (str): The path to the directory containing .txt files.

    Returns:
    str: Combined text of all files in the directory.
    """
    corpus = ""
    for filename in os.listdir(directory):
        if filename.endswith('.txt'):
            filepath = os.path.join(directory, filename)
            with open(filepath, 'r', encoding='utf-8') as file:
                text = file.read()
                corpus += text + " "
    return corpus

def vectorize_texts(max_features, analyzer, ngram_range, corpus_directory, reverse):
    """
    Vectorizes the text corpus into n-grams and returns their counts.

    Parameters:
    max_features (int): Maximum number of features to extract.
    analyzer (str): Type of n-gram ('word', 'char', 'char_wb').
    ngram_range (tuple): The range of n-gram values to use.
    corpus_directory (str): The directory containing the text corpus files.
    reverse (bool): Whether to sort the n-grams in descending order of frequency.

    Returns:
    tuple: List of counts and list of (n-gram, count) tuples sorted by frequency.
    """
    corpus = read_corpus(corpus_directory)

    vectorizer = CountVectorizer(max_features=max_features, analyzer=analyzer, ngram_range=ngram_range)
    X = vectorizer.fit_transform([corpus])

    ngram_counts = list(zip(vectorizer.get_feature_names_out(), X.toarray()[0]))
    ngram_counts_sorted = sorted(ngram_counts, key=lambda x: x[1], reverse=reverse)
    ngram_counts_sorted = [(word.replace(" ", "_"), count) for (word, count) in ngram_counts_sorted] #underscore to denote whitespace

    counts = [count for word, count in ngram_counts_sorted]

    return counts, ngram_counts_sorted

if __name__ == "__main__":
    counts, ngram_counts_sorted = vectorize_texts(
        max_features=10000, # upper limit features
        analyzer='char', # characters
        ngram_range=(4, 4), # 4-grams
        corpus_directory='../../corpora/corpus_imposters/',
        reverse=True # descending order
    )

    results_dir = os.path.join('..', '..', 'results', 'freq_dist_res')
    os.makedirs(results_dir, exist_ok=True) # create directory if it does not exist

    ax = sns.lineplot(data=counts)
    plt.axvline(2000, color='r', linestyle='dotted') # 2000th most frequent character 4gram
    ax.set_title("Frequency Distribution\nCharacter 4-grams (Verse Corpus Imposters)")
    ax.set_xlabel("Character 4-gram Index")
    ax.set_ylabel("Frequency")
    plt.savefig(os.path.join(results_dir, 'freq_dist_char_4grams.pdf'), dpi=500)
    plt.show()

    import pprint
    print("Top 100 most frequent 4-grams along with their overall frequency:\n")
    pprint.pprint(ngram_counts_sorted[:101], compact=True)



