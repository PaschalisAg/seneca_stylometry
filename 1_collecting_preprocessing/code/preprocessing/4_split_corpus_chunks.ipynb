{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f18289a-dc3c-4d44-bd04-5fe87736bfb5",
   "metadata": {},
   "source": [
    "The script below defines a class `TextChunkExtractor` designed to process Latin text files by splitting them into non-overlapping chunks based on a specified token count. The class performs preprocessing, tokenization, and chunking of texts, and saves the processed files into a designated directory.\n",
    "Key Components\n",
    "\n",
    "    Initialization:\n",
    "        `directory_to_read`: The directory containing the text files to be processed.\n",
    "        `directory_to_write`: The directory where the processed files will be saved. If it does not exist, it will be created.\n",
    "        `threshold_to_slice`: The minimum token count required to split the text into chunks.\n",
    "        `chunk_size`: The size of each non-overlapping chunk.\n",
    "\n",
    "    Methods:\n",
    "        `count_files(directory)`: Counts the number of .txt files in the given directory.\n",
    "        `read_file(filepath)`: Reads the content of a file.\n",
    "        `preprocess(text)`: Removes non-alphanumeric characters (except whitespace) from the text.\n",
    "        `tokenize_latin_text(text)`: Converts the text to lowercase and tokenizes it using CLTK's LatinWordTokenizer.\n",
    "        `extract_chunks()`: Processes each text file, splits it into chunks if it exceeds the threshold, and saves the chunks as separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7986d82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T19:23:51.205435Z",
     "start_time": "2023-04-30T19:23:47.652827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ../../corpora/corpus_kestemont_chunks/ already exists!\n",
      "\n",
      "        Every file has been written successfully.\n",
      "        The new directory (path=../../corpora/corpus_kestemont_chunks/) contains 6080 text samples.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from cltk.tokenizers import LatinWordTokenizer\n",
    "\n",
    "class TextChunkExtractor:\n",
    "    def __init__(self, directory_to_read, directory_to_write, threshold_to_slice, chunk_size):\n",
    "        \"\"\"\n",
    "        Initialize the TextChunkExtractor with directories and chunking parameters.\n",
    "\n",
    "        Parameters:\n",
    "            directory_to_read (str): The directory containing the texts to slice.\n",
    "            directory_to_write (str): The directory to write the results. If it doesn't exist, it will be created.\n",
    "            threshold_to_slice (int): The token count threshold above which texts will be split into chunks.\n",
    "            chunk_size (int): The size of each non-overlapping chunk.\n",
    "        \"\"\"\n",
    "        self.directory_to_read = Path(directory_to_read)\n",
    "        self.directory_to_write = Path(directory_to_write)\n",
    "        self.threshold_to_slice = threshold_to_slice\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "        # Create the directory if it does not exist\n",
    "        self.directory_to_write.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Directory {self.directory_to_write} is ready!\")\n",
    "\n",
    "    def count_files(self, directory):\n",
    "        \"\"\"Count the number of .txt files in the given directory.\"\"\"\n",
    "        return len(list(Path(directory).glob(\"*.txt\")))\n",
    "\n",
    "    def read_file(self, filepath):\n",
    "        \"\"\"Read the content of a file.\"\"\"\n",
    "        with filepath.open('r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Remove Arabic numbers from the text and return the cleaned text.\"\"\"\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    def tokenize_latin_text(self, text):\n",
    "        \"\"\"Lowercase and tokenize Latin text.\"\"\"\n",
    "        latin_tokenizer = LatinWordTokenizer()\n",
    "        text = self.preprocess(text.lower())\n",
    "        tokens = latin_tokenizer.tokenize(text)\n",
    "        return tokens\n",
    "\n",
    "    def extract_chunks(self):\n",
    "        \"\"\"Extract chunks from text files and save them in the specified directory.\"\"\"\n",
    "        # Process each file in the directory\n",
    "        for file_path in self.directory_to_read.glob(\"*.txt\"):\n",
    "            tokens = self.tokenize_latin_text(self.read_file(file_path))\n",
    "\n",
    "            if len(tokens) > self.threshold_to_slice:\n",
    "                chunks = [tokens[i:i + self.chunk_size] for i in range(0, len(tokens), self.chunk_size)]\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    chunk_text = \" \".join(chunk)\n",
    "                    chunk_file_name = f\"{file_path.stem}_chunk{i + 1}.txt\"\n",
    "                    with (self.directory_to_write / chunk_file_name).open(\"w\", encoding='utf-8') as f:\n",
    "                        f.write(chunk_text)\n",
    "            else:\n",
    "                text = \" \".join(tokens)\n",
    "                with (self.directory_to_write / file_path.name).open(\"w\", encoding='utf-8') as f:\n",
    "                    f.write(text)\n",
    "\n",
    "        print(f\"\"\"\n",
    "        Every file has been written successfully.\n",
    "        The new directory (path={self.directory_to_write}) contains {self.count_files(self.directory_to_write)} text samples.\"\"\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "directory_to_read = \"../../corpora/corpus_imposters/\"  # get the working directory\n",
    "directory_to_write = \"../../corpora/corpus_chunks/\"    # set the directory where you want to write the results\n",
    "extractor = TextChunkExtractor(directory_to_read, directory_to_write, 500, 500)\n",
    "extractor.extract_chunks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
