---
title: "R Notebook"
author: "Paschalis Agapitos"
output: pdf_document
---
# Introduction
Aim of this notebook is to validate the methods used in the paper *A Stylometric Analysis of Seneca's Disputed Plays: Authorship Verification of Octavia and Hercules Oetaeus*.

The validation dataset for the GI method includes the following authors and texts (we have removed the disputed plays by Seneca since these will part of the main analysis and not of the validation of the methods used):

* Marcus Annaeus Lucanus (i.e., Lucan):
  + *Pharsalia* (each book splitted into a separate txt file (10 in total))
  
* Marcus Manilius (i.e., Manilius):
  + *Astronomica* (each book splitted into a separate txt file (5 in total))
  
* Publius Ovidius Naso (i.e., Ovid):
  + *Amores*
  + *Ars Amatoria*
  + *Epistulae*
  + *Fasti*
  + *Ibis*
  + *Medicamina faciei femineae*
  + *Ex Ponto*
  + *Remedia Amoris*
  + *Tristia*
  
* Aulus Persius Flaccus (i.e., Persius):
  + *Satires* (each satire splitted into a separate txt file (6 in total))
  
* Gaius Julius Phaedrus (i.e., Phaedrus):
  + *Fabulae* (each book splitted into a separate txt file (6 in total))
  
* Lucius Annaeus Seneca Minor (i.e., Seneca the Younger):
  + *Agamemnon*
  + *Hercules Furens*
  + *Medea*
  + *Oedipus*
  + *Phaedra*
  + *Phoenissae*
  + *Thyestes*
  + *Troades*
  
* Silius Italicus (i.e., Silius):
  + *Punica* (each book splitted into a separate txt file (17 in total))
  
* Publius Papinius Statius (i.e., Statius):
  + *Achilleid*
  + *Silvae* (each book splitted into a separate txt file (5 in total))
  + *Thebaid* (each book splitted into a separate txt file (12 in total))
  
* Valerius Flaccus:
  + *Argonautica* (each book splitted into a separate txt file (8 in total))

* Martial:
  + *Epigrammata* (each book is split into a separate txt file (14 in total))


The same pre-processing step will be applied to every text that is used here and the results will be generated using Most Frequent Characters (henceforward MFCs) 4grams and pentagrams; these number of n-grams will be applied to each one of the aforementioned methods and their variations.

```{r}
set.seed(100) # random seed for reproducibility
```

# Importing the libraries

```{r}
# in case they are not installed or need to be update, uncomment and run
# install.packages("stylo")
# install.packages("writexl")
library(stylo)
library(writexl)
```
# Setting working directory

The first step that we need to take is to set the working directory to the folder where this notebook and our dataset are saved. In our case `Analysis` is our working directory.

```{r}
# Knit (click) > Knit Directory (click) > Current Working Directory [only if `setwd()` does not work] > run `setwd()`
setwd("../../../validation/validation_imposters")
getwd()
```

# Preparation of the dataset

The "preparation of the dataset" step consists of six steps.

1)  importing the corpus; at this step we need to make sure that we point out to the correct directory.
2)  tokenizing the corpus, lowering the case of the letters and removing punctuation marks. A very crucial detail on which we should pay attention is the the aspect of the language. Our dataset is written in Latin, so we could either use the `Latin` or `Latin.corr` in the parameter `lang`. Since a lot of manuscripts in Latin do not distinguish the letter `v` from the letter `u`, the second option suits better in this case.
3)  removing the pronouns; this step is optional but since some studies have shown that pronouns are related to the genre or content of a text, it was decided for them to be removed.
4)  extracting the features that we want to use; in our case character 4-grams.
5)  creating the frequency list
6)  creating the table with frequencies using the frequency list that was created in the previous step.

```{r}
# step 1
raw.corpus <- load.corpus(
  files = "all",
  corpus.dir = "../../validation/validation_corpora/validation_imp_corpus/",
  encoding = "UTF-8"
)
```

```{r}
# step 2
tokenized.corpus <- txt.to.words.ext(
    raw.corpus,
    corpus.lang = "Latin.corr",
    preserve.case = F,
)
```

```{r}
# step 3
corpus.no.pronouns <- delete.stop.words(
  tokenized.corpus,
  stop.words = stylo.pronouns(corpus.lang = "Latin.corr")
)
```


```{r}
# step 4
corpus.char.tetragrams <- txt.to.features(
  tokenized.text = corpus.no.pronouns,
  features = "c",
  ngram.size = 4
)
```

```{r}
# step 5
features_char_tetragrams <- make.frequency.list(
  corpus.char.tetragrams,
  head = 2000
)
```

```{r}
# step 6
data = make.table.of.frequencies(
  corpus = corpus.char.tetragrams,
  features = features_char_tetragrams,
  relative = T
)
```

# Implementation of `imposters` method
In order to implement `imposters` to the data that we have prepared in the previous steps we first need to find in which rows the disputed texts are. This will help us when we need to split the dataset into reference and test set; we could do that just by referring to the number of the row where the texts under investigation are.
Since are our dataset contains more than 999 rows we need to set the option for printing more than 999 rows. We have 1814 text samples, so if we set `max.option = 2000` it will output all the rows.

```{r}
options(max.print=105)
rownames(data)
```
# Optimize

The `imposters` method comes with an in-built function that optimizes the hyperparameters used in the General Imposters method. In other words it tries to define the grey area where these result could be valid or not.

```{r}
help("imposters.optimize")
optimized_parameters <- imposters.optimize(data)
```
```{r}
# the results come back in a vector, thus access by indexing
p1 <- optimized_parameters[1]
p2 <- optimized_parameters[2]
```

```{r}
# create an empty dataframe where we are going to save the results
results.imposters <- data.frame()

# define the test text as the current row
# apply the imposters method to compare the test text against the reference set
# every time imposters is applied to a different text in the corpus
# the disputed Senecan plays (i.e., Octavia and Hercules Oetaeus have been manually removed)
for (n in 1:nrow(data)) {
  
  test <- data[n, 1:2000]
  
  # define the reference set as all rows except for the current row
  reference.set <- data[-n, 1:2000]
  
  # extract the real author from the filename
  filename <- rownames(data)[n]
  real_author <- strsplit(filename, "_")[[1]][1]
  
  # set a random seed
  set.seed(n)
  
  # apply the imposters method to compare the test text against the reference set
  results <- imposters(reference.set = reference.set,
                       test = test,
                       iterations = 100,
                       distance = 'wurzburg')
  
  # find the attributed author (the one with the highest score)
  max_score <- max(results)
  attributed_author <- names(results)[which.max(results)]
  
  # append the result to the results dataframe
  results.imposters <- rbind(results.imposters,
                             data.frame(document = filename,
                                        real_author = real_author,
                                        attributed_author = attributed_author,
                                        score = max_score,
                                        p1_value = p1,
                                        p2_value = p2))
}

# check if the dataframe is empty
if (nrow(results.imposters) == 0) {
  print("No results found.")
} else {
  # view the results data frame
  print(results.imposters)
}

# write the data frame into an excel spreadsheet
write_xlsx(results.imposters, "../validation_imposters/results/results_imposters_valid_cosine.xlsx")
```